{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad21f9c4-15e0-4dcf-9f8a-00faf5ee1889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/roise0r/intellij-projects/yc-chatbot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1be2a-395d-4780-9b6f-d5aaf86a90a7",
   "metadata": {},
   "source": [
    "#### These are definitions responsible for pulling data from HackerNews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becddb4c-a931-4102-b7a7-872394532a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pytz\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import HTTPError, SSLError\n",
    "\n",
    "# tz_str = \"UTC\"\n",
    "tz_str = \"US/Pacific\"\n",
    "# tz_str = \"America/Los_Angeles\"\n",
    "os.environ['TZ'] = tz_str\n",
    "time.tzset()\n",
    "tz_ = pytz.timezone(tz_str)\n",
    "\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract text from the main content of the page.\n",
    "    # This may vary depending on the structure of the webpage.\n",
    "    paragraphs = soup.find_all('p')\n",
    "    page_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "    return page_text\n",
    "\n",
    "\n",
    "def fetch_hacker_news_stories(\n",
    "    store_every,\n",
    "    stories_cutoff_in_days,\n",
    "    min_chars_per_story,\n",
    "):\n",
    "    start_dt = datetime.now()\n",
    "    base_url = \"https://hacker-news.firebaseio.com/v0\"\n",
    "    max_item_id = requests.get(f\"{base_url}/maxitem.json\", timeout=10).json()\n",
    "\n",
    "    # Get the current time and calculate the cutoff time for the 'days' parameter.\n",
    "    current_time = datetime.now()\n",
    "    cutoff_time = current_time - timedelta(days=stories_cutoff_in_days)\n",
    "    # The cutoff day should be a full day, and included.\n",
    "    cutoff_time = cutoff_time.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    cutoff_timestamp = int(cutoff_time.timestamp())\n",
    "\n",
    "    stories = []\n",
    "    for item_id in range(max_item_id, 0, -store_every):\n",
    "\n",
    "        item_url = f\"{base_url}/item/{item_id}.json\"\n",
    "        try:\n",
    "            item_data = requests.get(item_url, timeout=10).json()\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"WARNING: Timeout for item_data for item_id: {item_id}\")\n",
    "            continue\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"WARNING: ConnectionError for item_data for item_id: {item_id}\")\n",
    "            continue\n",
    "\n",
    "        # Check if the item's time is less then the cutoff. If that's the case,\n",
    "        #   we can break early, since we know that any stories fetched from now\n",
    "        #   on will have a 'time' less than the cutoff as well, since we started\n",
    "        #   with the item with the largest item_id and going backwards.\n",
    "        if item_data['time'] < cutoff_timestamp:\n",
    "            print(\"Reached cutoff time.\")\n",
    "            break\n",
    "\n",
    "        # Skip this item if it's not a story or if it doesn't have an URL.\n",
    "        if item_data['type'] != 'story' or 'url' not in item_data:\n",
    "            continue\n",
    "\n",
    "        # Try fetching and extract text from the URL. If HTTPError is raised, skip.\n",
    "        try:\n",
    "            item_data['text'] = (\n",
    "                f\"Story Title: {item_data['title']} -- Story text: {extract_text_from_url(item_data['url'])}\"\n",
    "            )\n",
    "        except HTTPError:\n",
    "            print(f\"WARNING: HTTPError for story_id: {item_id}\")\n",
    "            continue\n",
    "        except SSLError:\n",
    "            print(f\"WARNING: SSLError for story_id: {item_id}\")\n",
    "            continue\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"WARNING: Timeout for extract_text for story_id: {item_id}\")\n",
    "            continue\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"WARNING: ConnectionError for extract_text for item_id: {item_id}\")\n",
    "            continue\n",
    "\n",
    "        # Include only stories that have at least some characters.\n",
    "        if len(item_data['text']) < min_chars_per_story:\n",
    "            print(f\"WARNING: Min char reqs not met for story_id: {item_id}\")\n",
    "            continue\n",
    "\n",
    "        # Add datetime field based on timestamp.\n",
    "        item_data[\"dt\"] = datetime.fromtimestamp(item_data[\"time\"], tz=tz_).isoformat()\n",
    "\n",
    "        stories.append(item_data)\n",
    "\n",
    "        # Checkpoint.\n",
    "        if len(stories) % 10 == 0:\n",
    "            print(f\"Stories fetched so far: {len(stories)}\")\n",
    "\n",
    "    print(f\"Total stories fetched: {len(stories)}\")\n",
    "    print(f\"Runtime: {datetime.now() - start_dt}\")\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baed32f-12cf-4be6-8e8b-e5399ad9dcd7",
   "metadata": {},
   "source": [
    "#### The main method for the definitions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aace40a-abd1-4ed8-b2db-e504b34fa53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached cutoff time.\n",
      "Total stories fetched: 2\n",
      "Runtime: 0:00:14.654571\n",
      "Stories written to hacker_news_stories.json\n"
     ]
    }
   ],
   "source": [
    "## args being (we could use `argparse`, but no point).\n",
    "\n",
    "# In order to be able to finish fetching stories for all days in a reasonable\n",
    "#   amount of time, we need to only fetch every other story or so.\n",
    "store_every = 1\n",
    "# Get stories for this amount of days in the past, including today. Last day\n",
    "#   will have fewer stories than other days.\n",
    "stories_cutoff_in_days = 3\n",
    "# Don't save stories that have fewer than 100 chars.\n",
    "min_chars_per_story = 100\n",
    "\n",
    "## args end\n",
    "\n",
    "hacker_news_stories = fetch_hacker_news_stories(\n",
    "    store_every=store_every,\n",
    "    stories_cutoff_in_days=stories_cutoff_in_days,\n",
    "    min_chars_per_story=min_chars_per_story,\n",
    ")\n",
    "\n",
    "file_path = 'hacker_news_stories.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    # noinspection PyTypeChecker\n",
    "    json.dump(hacker_news_stories, json_file)\n",
    "print(f\"Stories written to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b7955-e82a-42e8-a440-1228ca2807ae",
   "metadata": {},
   "source": [
    "#### Next is store_embeddings.py -- It's responsible for getting all the info from the JSON we created and ingest all that into chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d22753-3c8f-40cf-a8db-01b993f3695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roise0r/Library/Caches/pypoetry/virtualenvs/yc-chatbot-Yp3Islnz-py3.10/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package stopwords to /Users/roise0r/intellij-\n",
      "[nltk_data]     projects/yc-chatbot...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import chromadb\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from modules.memory import Memory\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Load stop words. this needs to be outside functions so it doesn't download each time. duh!\n",
    "nltk.download('stopwords', download_dir=os.getcwd())\n",
    "nltk.data.path = [os.getcwd()]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    cleaned_text = text.lower()\n",
    "\n",
    "    cleaned_text = cleaned_text.replace(\"story title: \", \" \")\n",
    "    cleaned_text = cleaned_text.replace(\"story text: \", \" \")\n",
    "    cleaned_text = cleaned_text.replace(\"--\", \" \")\n",
    "\n",
    "    cleaned_text = ' '.join(_ for _ in cleaned_text.split() if _ not in stop_words)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def generate_embeddings(data):\n",
    "\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    cleaned_texts = [clean_text(story['text']) for story in data]\n",
    "    embeddings = model.encode(cleaned_texts, show_progress_bar=True)\n",
    "    return cleaned_texts, embeddings\n",
    "\n",
    "\n",
    "def store_embeddings(hacker_news_stories, use_cleaned_text, cleaned_texts, embeddings):\n",
    "\n",
    "    chromadb_client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "\n",
    "    # Check if the collection already exists and drop it\n",
    "    if \"hacker_news\" in [collection.name for collection in chromadb_client.list_collections()]:\n",
    "        chromadb_client.delete_collection(\"hacker_news\")\n",
    "    collection = chromadb_client.create_collection(\"hacker_news\")\n",
    "\n",
    "    for i, (story, cleaned_text, embedding) in enumerate(zip(hacker_news_stories, cleaned_texts, embeddings)):\n",
    "        collection.add(\n",
    "            metadatas=[{\n",
    "                \"date\": story[\"dt\"][:10],\n",
    "                \"timestamp\": story[\"time\"],\n",
    "            }],\n",
    "            documents=[cleaned_text if use_cleaned_text else story['text']],\n",
    "            embeddings=[embedding.tolist()],\n",
    "            ids=[f\"id_{i}\"],\n",
    "        )\n",
    "\n",
    "    # Check some embeddings.\n",
    "    embeddings_ls = [embedding.tolist() for embedding in embeddings]\n",
    "    # n_results=10 is only for sanity-checking the saved stories, so it doesn't matter.\n",
    "    results = collection.query(query_embeddings=[embeddings_ls[0]], n_results=10)\n",
    "    print(json.dumps(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a515f64-c994-4ac5-9089-5bf4283c009f",
   "metadata": {},
   "source": [
    "#### and its main method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d7ae4-ca77-4f6c-ac91-f9e724696790",
   "metadata": {},
   "outputs": [],
   "source": [
    "## args being\n",
    "\n",
    "# Whether to use the cleaned text (remove stop words, lowecase, etc.), or use original text to store in chromadb.\n",
    "use_cleaned_text = False\n",
    "\n",
    "## args end\n",
    "\n",
    "memory = Memory()\n",
    "memory.log_memory(print, \"before\")\n",
    "dt_start = datetime.now()\n",
    "\n",
    "file_path = 'hacker_news_stories.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    hacker_news_stories = json.load(file)\n",
    "    print(f\"Total stories to be stored: {len(hacker_news_stories)}\")\n",
    "\n",
    "cleaned_texts, embeddings = generate_embeddings(hacker_news_stories)\n",
    "memory.log_memory(print, \"after_generate\")\n",
    "\n",
    "print(f\"type embeddings: {type(embeddings)}\")\n",
    "print(f\"len embeddings: {len(embeddings)}\")\n",
    "print(f\"size embeddings: {embeddings.size}\")\n",
    "print(f\"embeddings: {embeddings}\")\n",
    "print(f\"len embeddings 0: {len(embeddings[0])}\")\n",
    "print(f\"len embeddings 1: {len(embeddings[1])}\")\n",
    "print(f\"len embeddings 2: {len(embeddings[2])}\")\n",
    "\n",
    "store_embeddings(\n",
    "    hacker_news_stories,\n",
    "    use_cleaned_text,\n",
    "    cleaned_texts,\n",
    "    embeddings,\n",
    ")\n",
    "memory.log_memory(print, \"after_store\")\n",
    "\n",
    "print(f\"Total runtime: {datetime.now() - dt_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ced1c-8daf-41ee-ba17-28e2e5a2ff22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
