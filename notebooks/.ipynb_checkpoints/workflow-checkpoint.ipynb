{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad21f9c4-15e0-4dcf-9f8a-00faf5ee1889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/roise0r/intellij-projects/yc-chatbot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1be2a-395d-4780-9b6f-d5aaf86a90a7",
   "metadata": {},
   "source": [
    "#### These are definitions responsible for pulling data from HackerNews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becddb4c-a931-4102-b7a7-872394532a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pytz\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import HTTPError, SSLError\n",
    "\n",
    "# tz_str = \"UTC\"\n",
    "tz_str = \"US/Pacific\"\n",
    "# tz_str = \"America/Los_Angeles\"\n",
    "os.environ['TZ'] = tz_str\n",
    "time.tzset()\n",
    "tz_ = pytz.timezone(tz_str)\n",
    "\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract text from the main content of the page.\n",
    "    # This may vary depending on the structure of the webpage.\n",
    "    paragraphs = soup.find_all('p')\n",
    "    page_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "    return page_text\n",
    "\n",
    "\n",
    "def fetch_hacker_news_stories(\n",
    "    store_every,\n",
    "    stories_cutoff_in_days,\n",
    "    min_chars_per_story,\n",
    "):\n",
    "    start_dt = datetime.now()\n",
    "    base_url = \"https://hacker-news.firebaseio.com/v0\"\n",
    "    max_item_id = requests.get(f\"{base_url}/maxitem.json\", timeout=10).json()\n",
    "\n",
    "    # Get the current time and calculate the cutoff time for the 'days' parameter.\n",
    "    current_time = datetime.now()\n",
    "    cutoff_time = current_time - timedelta(days=stories_cutoff_in_days)\n",
    "    # The cutoff day should be a full day, and included.\n",
    "    cutoff_time = cutoff_time.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    cutoff_timestamp = int(cutoff_time.timestamp())\n",
    "\n",
    "    stories = []\n",
    "    for item_id in range(max_item_id, 0, -store_every):\n",
    "\n",
    "        item_url = f\"{base_url}/item/{item_id}.json\"\n",
    "        try:\n",
    "            item_data = requests.get(item_url, timeout=10).json()\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"WARNING: Timeout for item_data for item_id: {item_id}\")\n",
    "            continue\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"WARNING: ConnectionError for item_data for item_id: {item_id}\")\n",
    "            continue\n",
    "\n",
    "        # Check if the item's time is less then the cutoff. If that's the case,\n",
    "        #   we can break early, since we know that any stories fetched from now\n",
    "        #   on will have a 'time' less than the cutoff as well, since we started\n",
    "        #   with the item with the largest item_id and going backwards.\n",
    "        if item_data['time'] < cutoff_timestamp:\n",
    "            print(\"Reached cutoff time.\")\n",
    "            break\n",
    "\n",
    "        # Skip this item if it's not a story or if it doesn't have an URL.\n",
    "        if item_data['type'] != 'story' or 'url' not in item_data:\n",
    "            continue\n",
    "\n",
    "        # Try fetching and extract text from the URL. If HTTPError is raised, skip.\n",
    "        try:\n",
    "            item_data['text'] = (\n",
    "                f\"Story Title: {item_data['title']} -- Story text: {extract_text_from_url(item_data['url'])}\"\n",
    "            )\n",
    "        except HTTPError:\n",
    "            print(f\"WARNING: HTTPError for story_id: {item_id}\")\n",
    "            continue\n",
    "        except SSLError:\n",
    "            print(f\"WARNING: SSLError for story_id: {item_id}\")\n",
    "            continue\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"WARNING: Timeout for extract_text for story_id: {item_id}\")\n",
    "            continue\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"WARNING: ConnectionError for extract_text for item_id: {item_id}\")\n",
    "            continue\n",
    "\n",
    "        # Include only stories that have at least some characters.\n",
    "        if len(item_data['text']) < min_chars_per_story:\n",
    "            print(f\"WARNING: Min char reqs not met for story_id: {item_id}\")\n",
    "            continue\n",
    "\n",
    "        # Add datetime field based on timestamp.\n",
    "        item_data[\"dt\"] = datetime.fromtimestamp(item_data[\"time\"], tz=tz_).isoformat()\n",
    "\n",
    "        stories.append(item_data)\n",
    "\n",
    "        # Checkpoint.\n",
    "        if len(stories) % 10 == 0:\n",
    "            print(f\"Stories fetched so far: {len(stories)}\")\n",
    "\n",
    "    print(f\"Total stories fetched: {len(stories)}\")\n",
    "    print(f\"Runtime: {datetime.now() - start_dt}\")\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baed32f-12cf-4be6-8e8b-e5399ad9dcd7",
   "metadata": {},
   "source": [
    "#### The main method for the definitions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aace40a-abd1-4ed8-b2db-e504b34fa53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached cutoff time.\n",
      "Total stories fetched: 2\n",
      "Runtime: 0:00:14.654571\n",
      "Stories written to hacker_news_stories.json\n"
     ]
    }
   ],
   "source": [
    "## args being (we could use `argparse`, but no point).\n",
    "\n",
    "# In order to be able to finish fetching stories for all days in a reasonable\n",
    "#   amount of time, we need to only fetch every other story or so.\n",
    "store_every = 1\n",
    "# Get stories for this amount of days in the past, including today. Last day\n",
    "#   will have fewer stories than other days.\n",
    "stories_cutoff_in_days = 3\n",
    "# Don't save stories that have fewer than 100 chars.\n",
    "min_chars_per_story = 100\n",
    "\n",
    "## args end\n",
    "\n",
    "hacker_news_stories = fetch_hacker_news_stories(\n",
    "    store_every=store_every,\n",
    "    stories_cutoff_in_days=stories_cutoff_in_days,\n",
    "    min_chars_per_story=min_chars_per_story,\n",
    ")\n",
    "\n",
    "file_path = 'hacker_news_stories.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    # noinspection PyTypeChecker\n",
    "    json.dump(hacker_news_stories, json_file)\n",
    "print(f\"Stories written to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c495f4f0-188e-429a-a8dc-410aa2d5cd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
